---
title: "Pre-processing"
author: "Julia Schwarz"
date: "28/10/2021"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## Packages

```{r packages, include=FALSE}
library(lme4)
library(ggplot2)
library(lattice)
library(car)
library(lmerTest)
library(moments)
library(rcompanion)
library(MuMIn)
library(Hmisc)
library(dplyr)
library(stargazer)
```

## Some updates compared to the previous version (to be discussed):
- I moved 'individual inspection' section as part of participant screening here.
- I reincluded c135, as there were probably worse ones than that.
- I dont' think we should exclude trials lower than 0 ms - because it is very likely to happen if a participants just predict and respond. For log transformation, we can add a small value to it and make all data positive. 
- We probably need more detailed inspection over individual data.

## Data File

From first batch of files: 17 adults and 17 children
From second batch of files: 

16 children and 14 adults

Raw data file with all 1-17 adults + 1-17 children, no data removed except empty responses. **7871** observations. 

```{r load data, echo=FALSE}
data<-read.csv("fulldata.csv", header=T) 
nrow(data)
```

## Variable Prep

```{r variables, echo=FALSE}

# Random 
data$SUBJECT <- as.factor(data$parid)
data$ITEM <- as.factor(data$number)

# Outcome Var
data$RT <- as.numeric(data$react2) #RT from the end of the sentence

# Fixed
data$GROUP <- as.factor(data$group) #adults vs children
data$AUDIO <- as.factor(data$audio) 
data$VISUAL <- as.factor(data$video)
data$PREDICT <- as.factor(data$context)
#data$TRIALORDER <- as.numeric(data$TRIALORDER)

# Other
data$ACC <- as.factor(data$accuracy) #1==correct
data$CONDITION <- as.factor(data$type)
#data$BLOCK <- as.factor(data$BLOCK)
#data$LIST <- as.factor(data$Experiment_version)

```

## Exclude training trials

8 training trials per participant excluded. Remaining data = **7394**.

```{r training}
data <- subset(data, data$ITEM != '101L')
data <- subset(data, data$ITEM != '48L')
data <- subset(data, data$ITEM != '44L')
data <- subset(data, data$ITEM != '45H')
data <- subset(data, data$ITEM != '13H')
data <- subset(data, data$ITEM != '102L')
data <- subset(data, data$ITEM != '55H')
data <- subset(data, data$ITEM != '52H')
print("Remaining data: ")
nrow(data)
```

## Participant screening

Based on the post-experiment questionnaire, we excluded participants who do not match our recruitment criterial despite pre-screening (e.g. dyslexia, cf. Data collection procedures). 

Based on the quality of recording, we excluded participants reporting low commitment to the task (irrelevant speaking, background noise), any major misunderstandings of the instructions, or significant technical problems (e.g. with the experiment display)

Remaning darta: **7274**.

```{r participants, echo=FALSE}
data <- subset(data, data$SUBJECT != '210725_a180')
print("Remaining data: ")
nrow(data)
```

The rest filtering are based on data patterns.

### Based on the total number of trials
There are four children whose usable datapoints are less than 100, indicating the technical problems might be severe. We need to check these individual data to see distributions.
c43:79
c45:82
c96:75
c98:72

**c43, c45, c96, c98 need inspection**

```{r}
data %>%
  group_by(GROUP, parid) %>%
  summarise(count = n()) %>%
  filter(count<100)
```


### Based on the accuracy
We will exclude participants with an error rate above 20% . 

Participants with error rate above 20%: NONE
Participants with error rate above 10%: c29,c135

**c29 and c135 needs inspection**


```{r}
data %>%
  group_by(parid) %>%
  summarise(errorrate = (1-sum(accuracy)/n())*100) %>%
  filter(errorrate > 10)
```

## Items and trials (exclude items w high error rate)

Target words with an error rate of 40% or higher (cf. Crepaldi et al. 2016) will be removed from the analysis. 

Items w 40% error rate or higher: NONE
Items w higher than 20% error rate: 
28L: 20.7%
64L: 21.4%
86L: 20.6%
```{r}
data %>%
  group_by(ITEM) %>%
  summarise(errorrate = 100*(1-sum(accuracy)/n())) %>%
  filter(errorrate > 20)
```

## Exclude incorrect trials

Incorrect and problematic trials were removed (N=164, 4.3%). Remaining data = **6961**.
We will analyse wrong trials separately.

```{r incorrect}
data <- subset(data, data$ACC != 0)
print("wrong trials:")
7274-6961
print("error rate")
100*(7274-6961)/7274
```



[[In addition, we might remove all trials where the target was ambiguous (charity meal, knob, a net); for now, none removed]].

```{r items, echo=FALSE}
itemnumbers <- groupwiseMean(RT ~ ITEM,
                           data   = data,
                           conf   = 0.95,
                           digits = 3)
#data <- subset(data, data$ITEM !='...') 
```



## Inspect data and remove unnatural responses
```{r}
range(data$RT)
data$RTcut = cut(data$RT,breaks = c(-50,1000,3000,5000,20000))
table(data$RTcut)
```

Unnatural reaction times below 0ms and above 3000ms were removed since they indicate insufficient attention (N=22; Fairs & Strijkers allow 2000ms for picture naming; Vogt, Hauber, Kuhlen & Rahman allowed 3000ms for picture naming from onset; Kessler, Treiman & Mullennix allowed 2000ms for word reading). 

After this cleaning process, reaction times will be inspected with histograms, with boxplots, and by Participant with qqplot. 
Data remaining: 3606.

```{r outliers, echo=FALSE}
data <- subset(data, data$RT > 0) #1 trial removed
data <- subset(data, data$RT < 3000) #15 trials removed
3621-3606

hist(data$RT, breaks = 30)
boxplot(data$RT)
skewness(data$RT, na.rm = TRUE) # 0.8570 slightly positively skewed

qqmath(~RT|SUBJECT, data=data, main="Responses by Subject")

```



## Transform reaction times and remove outliers

The data were found to be positively skewed. Therefore we tested common transformations (actually only tested log so far) and chose the one with the best fit/but found no satisfactory improvement. Outliers three standard deviations from the mean per participant were removed.

Data remaining: 3560.

```{r transformation, echo=FALSE}

data$LogRT <- log(data$RT)
data$LogRT <- as.numeric(data$LogRT)
hist(data$LogRT)
boxplot(data$LogRT)
skewness(data$LogRT, na.rm = TRUE) #-0.96 = worse fit

outliers <- data %>% group_by(SUBJECT) %>% filter(!(abs(RT - mean(RT)) < 3*sd(RT))) #46 obs.
data <- data %>% group_by(SUBJECT) %>% filter(!(abs(RT - mean(RT)) > 3*sd(RT)))

skewness(data$RT, na.rm = TRUE) #0.83
hist(data$RT, breaks = 30)
qqmath(~RT|SUBJECT, data=data, main="Responses by Subject")

```



## Save cleaned data sheet

```{r save}
write.table(data, "220103_full_data_cleaned.txt", sep="\t", row.names=FALSE)
```



