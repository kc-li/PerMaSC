---
title: "Pre-processing"
author: "Julia Schwarz"
date: "28/10/2021"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## Packages

```{r packages, include=FALSE}
library(lme4)
library(ggplot2)
library(lattice)
library(car)
library(lmerTest)
library(moments)
library(rcompanion)
library(MuMIn)
library(Hmisc)
library(dplyr)
library(stargazer)
```

## Some updates compared to the previous version (to be discussed):
- I reincluded c135, as there were probably worse ones than that.
- Reaction time lower than 0 ms: I dont' think we should exclude trials lower than 0 ms - because it is very likely to happen if a participants just predict and respond. For log transformation, we can add a small value to it and make all data positive. 
- We probably need more detailed inspection over individual data.

## Data File

From first batch of files: 17 adults and 17 children
From second batch of files: 

16 children and 14 adults

Raw data file with all 1-17 adults + 1-17 children, no data removed except empty responses. **7870** observations. 

```{r load data, echo=FALSE}
data<-read.csv("fulldata.csv", header=T) 
print("Total data:")
nrow(data)
```

## Variable Prep
New variables here: *TRIAL*

```{r variables, echo=FALSE}

# Random 
data$SUBJECT <- as.factor(data$parid)
data$ITEM <- as.factor(data$number)

# Outcome Var
data$RT <- as.numeric(data$react2) #RT from the end of the sentence

# Fixed
data$GROUP <- as.factor(data$group) #adults vs children
data$AUDIO <- as.factor(data$audio) 
data$VISUAL <- as.factor(data$video)
data$PREDICT <- as.factor(data$context)
#data$TRIALORDER <- as.numeric(data$TRIALORDER)

# Other
data$ACC <- as.factor(data$accuracy) #1==correct
data$CONDITION <- as.factor(data$type)
data$TRIAL <- as.numeric(data$row_number) 
data$BLOCK <- as.factor(data$block)
#### Age not finished!
data$AGE <- as.numeric(data$date.of.birth.year + data$date.of.birth.month/12)
data$SEX <- as.factor(data$sex)

#data$LIST <- as.factor(data$Experiment_version)

data[!duplicated(data$parid),] %>%
  group_by(GROUP) %>%
  summarize(n = n())
```

## Exclude training trials

8 training trials per participant excluded. Remaining data = **7393**.

```{r training}
data <- data %>%
  filter(display != "Practice") # the same as below
data <- subset(data, data$ITEM != '101L')
data <- subset(data, data$ITEM != '48L')
data <- subset(data, data$ITEM != '44L')
data <- subset(data, data$ITEM != '45H')
data <- subset(data, data$ITEM != '13H')
data <- subset(data, data$ITEM != '102L')
data <- subset(data, data$ITEM != '55H')
data <- subset(data, data$ITEM != '52H')
print("Remaining data: ")
nrow(data)
```

## Participant screening
Based on the quality of recording, we excluded participants reporting low commitment to the task (irrelevant speaking, background noise), any major misunderstandings of the instructions, or significant technical problems (e.g. with the experiment display)

Based on the post-experiment questionnaire, we excluded participants who do not match our recruitment criterial despite pre-screening (e.g. dyslexia, cf. Data collection procedures). 
**Exclude a180 - dyalexia; 210725_a113 - effot doubtful; 210719_a59**

```{r participants, echo=FALSE}
data <- subset(data, data$SUBJECT != '210725_a180')
data <- subset(data, data$SUBJECT != '210725_a113')
data <- subset(data, data$SUBJECT != '210719_a59')
print("Remaining data: ")
nrow(data)

```

The rest filtering are based on data patterns.

### Based on the total number of trials
There are four children whose usable datapoints are less than 100, indicating the technical problems might be severe. We need to check these individual data to see distributions.
210719_c43:79
210719_c45:82
210725_c96:75
210725_c98:72

**c43, c45, c96, c98 not enough data**
**Exclude c96: Age 13 (75 reponses)**

Remaining data: **6961**
```{r}
data %>%
  group_by(GROUP, parid) %>%
  summarise(count = n()) %>%
  filter(count<100)

data <- subset(data, data$SUBJECT != '210725_c96')
print("Remaining data: ")
nrow(data)
data[!duplicated(data$parid),] %>%
  group_by(GROUP) %>%
  summarize(n = n())
```
```{r}
# Check if adult & children match
data[!duplicated(data$parid),] %>%
  group_by(GROUP) %>%
  summarize(n = n())
# Check if sex match
data[!duplicated(data$parid),] %>%
  group_by(GROUP,SEX) %>%
  summarize(n = n())
```


### Based on the accuracy
We will exclude participants with an error rate above 20% . 

Participants with error rate above 20%: NONE
Participants with error rate above 10%: c29,c135

**c29 and c135 needs inspection**
**No exclusion**


```{r}
data %>%
  group_by(parid) %>%
  summarise(errorrate = (1-sum(accuracy)/n())*100) %>%
  filter(errorrate > 10)
```

## Items and trials (exclude items w high error rate)

Target words with an error rate of 40% or higher (cf. Crepaldi et al. 2016) will be removed from the analysis. 

Items w 40% error rate or higher: NONE
Items w higher than 20% error rate: 
64L: 21.4%
21L: 20.6%
86L: 20.6%

**No targets removed based on error rate. **
**We excluded three ambiguous target words: knob, meal, net.**
Remaining trials: **6785**
```{r}
data %>%
  group_by(ITEM) %>%
  summarise(errorrate = 100*(1-sum(accuracy)/n())) %>%
  filter(errorrate > 20)
data <- subset(data, data$correctans != 'knob')
data <- subset(data, data$correctans != 'meal')
data <- subset(data, data$correctans != 'net')
data %>%
  filter(correctans == "knob")

print("Remaining trials:")
nrow(data)
```


## Exclude incorrect trials

Incorrect and problematic trials were removed (N=164, 4.3%). Remaining data = **6758**.
We will analyse wrong trials separately.

In the correct responses (ACC ==1), we delete the ones with 'discard' marking, because it's not possible to mark the speech onset accurately, but we keep the ones with 'unsure' marking (including participants sounding unsure; and coder unsure because of overlapping sound.)
```{r}
#7 discard in ACC==1 -> accurate
## Take out
data %>%
  filter(ACC ==1) %>%
  filter(grepl('discard',note))

# Keep them all
data %>%
  filter(ACC ==1) %>%
  filter(grepl('unsure',note))

data <- subset(data, data$ACC !=1 | !grepl('discard',note))
print("Remaining trials:")
nrow(data)
x = nrow(data)

```
```{r error data}
error <- subset(data, data$ACC!=1)
write.csv(error,"220107_unfiltered_error.csv")
```

```{r incorrect}
data <- subset(data, data$ACC != 0)
print("wrong trials:")
x-nrow(data)
print("error rate")
100*(x-nrow(data))/x
print("Remaining trials")
nrow(data)
```


```{r items, echo=FALSE}
# We checked for each item, we have enough data (18-38)
itemnumbers <- groupwiseMean(RT ~ ITEM,
                           data   = data,
                           conf   = 0.95,
                           digits = 3)
#data <- subset(data, data$ITEM !='...') 
```



## Inspect data and remove unnatural responses
```{r}
range(data$RT)
data$RTcut = cut(data$RT,breaks = c(-500,-50,0,1000,3000,20000))
table(data$RTcut)

data %>%
  filter(RT<0)
```

Unnatural reaction times below 0ms and above 3000ms were removed since they indicate insufficient attention (N=22; Fairs & Strijkers allow 2000ms for picture naming; Vogt, Hauber, Kuhlen & Rahman allowed 3000ms for picture naming from onset; Kessler, Treiman & Mullennix allowed 2000ms for word reading). 

After this cleaning process, reaction times will be inspected with histograms, with boxplots, and by Participant with qqplot. 
Data remaining: **6477**.

```{r outliers, echo=FALSE}
data <- subset(data, data$RT < 3000) #29 trials removed
print("Remaining trials:")
nrow(data)

hist(data$RT, breaks = 30)
boxplot(data$RT)
skewness(data$RT, na.rm = TRUE) # 0.8570 slightly positively skewed

qqmath(~RT|SUBJECT, data=data, main="Responses by Subject")

```

### Distraction and Difficulty (commitment to the experiment)
Difficulty.y >3: **210723_a78** (Difficulty 5, Distraction 3)
Distraction >3: **210725_c133** (Difficulty 2, Disctraction 4)

**Nothing excluded**
#Check adding CI
https://stackoverflow.com/questions/35953394/calculating-length-of-95-ci-using-dplyr
```{r}
# remove duplicated, only consider one line per participant
# CI interval
diffi_distr <- data[!duplicated(data$parid),] %>%
  summarise(difficulty.mean = mean(Difficulty.y), diffculty.min = min(Difficulty.y), diffculty.max = max(Difficulty.y), distraction.mean = mean(Distraction),distraction.min = min(Distraction), distraction.max = max(Distraction))


Qpart <- data[!duplicated(data$parid),]

# difficultymean <- groupwiseMean(Difficulty.y,
#                            data   = Qpart,
#                            conf   = 0.95,
#                            digits = 3)
  
data[!duplicated(data$parid),] %>%
  filter(Difficulty.y > 3)

data[!duplicated(data$parid),] %>%
  filter(Distraction > 3)

#hist(data[data$parid == "210725_c133","RT"], breaks = 30)
#hist(data[data$parid == "210719_c08","RT"], breaks = 30)
#hist(data[data$parid == "210725_c81","RT"], breaks = 30)
```


## Transform reaction times and remove outliers

The data were found to be positively skewed. Therefore we tested common transformations (actually only tested log so far) and chose the one with the best fit/but found no satisfactory improvement. Outliers three standard deviations from the mean per participant were removed.

Data remaining: 3560.

```{r transformation, echo=FALSE}
skewness(data$RT, na.rm = TRUE)
hist(data$RT, breaks = 30)
data$LogRT <- log(data$RT)
data$LogRT <- as.numeric(data$LogRT)
data$RT500 <- data$RT + 500
data$logRT500 <- log(data$RT500)
data$logRT500 <- as.numeric(data$logRT500)
hist(data$LogRT,breaks = 30)
hist(data$logRT500,breaks = 30)
boxplot(data$LogRT)
skewness(data$LogRT, na.rm = TRUE) #-0.96 = worse fit
skewness(data$logRT500, na.rm = TRUE) #-0.96 = worse fit

outliers <- data %>% group_by(SUBJECT) %>% filter(!(abs(RT - mean(RT)) < 3*sd(RT))) #92 obs.
data <- data %>% group_by(SUBJECT) %>% filter(!(abs(RT - mean(RT)) > 3*sd(RT)))

skewness(data$RT, na.rm = TRUE) #1.4275
hist(data$RT, breaks = 30)
qqmath(~RT|SUBJECT, data=data, main="Responses by Subject")

```



## Save cleaned data sheet

```{r save}
write.table(data, "220107_full_data_cleaned.txt", sep="\t", row.names=FALSE)
```


